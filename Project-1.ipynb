{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78fd6f68-23bf-4388-b524-69718b0e25eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "716910b0-5bf1-42a8-993b-35c56be58cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Data Preparation and Preprocessing\n",
      "----------------------------------------\n",
      "Data preparation complete.\n",
      "X_train shape: (7992, 58, 1)\n",
      "X_test shape: (1998, 58, 1)\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Data Preparation and Preprocessing ---\n",
    "\n",
    "print(\"Step 1: Data Preparation and Preprocessing\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    features_df = pd.read_csv(\"features_3_sec.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'features_3_sec.csv' not found. Please ensure the file is in the same directory as the script.\")\n",
    "    exit()\n",
    "\n",
    "# Drop the 'filename' column as it's not a feature for the model\n",
    "if 'filename' in features_df.columns:\n",
    "    features_df.drop('filename', axis=1, inplace=True)\n",
    "\n",
    "# Handle missing values by dropping rows with NaN\n",
    "features_df.dropna(inplace=True)\n",
    "\n",
    "# Encode the labels from text to numbers\n",
    "label_encoder = LabelEncoder()\n",
    "features_df['label'] = label_encoder.fit_transform(features_df['label'])\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = features_df.drop('label', axis=1)\n",
    "y = features_df['label']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Reshape data for the CNN model\n",
    "X_train_cnn = np.expand_dims(X_train, axis=2)\n",
    "X_test_cnn = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "print(\"Data preparation complete.\")\n",
    "print(\"X_train shape:\", X_train_cnn.shape)\n",
    "print(\"X_test shape:\", X_test_cnn.shape)\n",
    "print(\"\\n\" + \"=\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "948a8867-820f-47c7-bd3d-a3880e892451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Building and Training the CNN Model\n",
      "----------------------------------------\n",
      "Starting CNN model training...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda_projects\\Newclnda\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.4967 - loss: 1.4224 - val_accuracy: 0.6637 - val_loss: 1.0316\n",
      "Epoch 2/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.6695 - loss: 0.9780 - val_accuracy: 0.7167 - val_loss: 0.8422\n",
      "Epoch 3/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.7320 - loss: 0.7831 - val_accuracy: 0.7588 - val_loss: 0.7128\n",
      "Epoch 4/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.7804 - loss: 0.6491 - val_accuracy: 0.7863 - val_loss: 0.6249\n",
      "Epoch 5/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8174 - loss: 0.5382 - val_accuracy: 0.8143 - val_loss: 0.5781\n",
      "Epoch 6/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8411 - loss: 0.4620 - val_accuracy: 0.8288 - val_loss: 0.5302\n",
      "Epoch 7/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.8719 - loss: 0.3756 - val_accuracy: 0.8353 - val_loss: 0.5071\n",
      "Epoch 8/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8814 - loss: 0.3427 - val_accuracy: 0.8418 - val_loss: 0.4772\n",
      "Epoch 9/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8994 - loss: 0.2851 - val_accuracy: 0.8453 - val_loss: 0.4922\n",
      "Epoch 10/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9150 - loss: 0.2454 - val_accuracy: 0.8559 - val_loss: 0.4957\n",
      "Epoch 11/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9224 - loss: 0.2262 - val_accuracy: 0.8624 - val_loss: 0.4873\n",
      "Epoch 12/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9288 - loss: 0.2046 - val_accuracy: 0.8649 - val_loss: 0.4700\n",
      "Epoch 13/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9332 - loss: 0.1917 - val_accuracy: 0.8624 - val_loss: 0.4917\n",
      "Epoch 14/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9477 - loss: 0.1543 - val_accuracy: 0.8684 - val_loss: 0.5048\n",
      "Epoch 15/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9494 - loss: 0.1472 - val_accuracy: 0.8739 - val_loss: 0.5038\n",
      "Epoch 16/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9525 - loss: 0.1382 - val_accuracy: 0.8729 - val_loss: 0.4749\n",
      "Epoch 17/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9562 - loss: 0.1295 - val_accuracy: 0.8744 - val_loss: 0.4981\n",
      "Epoch 18/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.9628 - loss: 0.1139 - val_accuracy: 0.8734 - val_loss: 0.4834\n",
      "Epoch 19/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9648 - loss: 0.1096 - val_accuracy: 0.8779 - val_loss: 0.5063\n",
      "Epoch 20/20\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9638 - loss: 0.1041 - val_accuracy: 0.8764 - val_loss: 0.4968\n",
      "CNN model training complete.\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Building and Training the CNN Model ---\n",
    "\n",
    "print(\"Step 2: Building and Training the CNN Model\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "num_features = X_train_cnn.shape[1]\n",
    "num_labels = len(np.unique(y))\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(num_features, 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_labels, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Starting CNN model training...\")\n",
    "model.fit(X_train_cnn, y_train, epochs=20, batch_size=32, validation_data=(X_test_cnn, y_test))\n",
    "print(\"CNN model training complete.\")\n",
    "print(\"\\n\" + \"=\" * 40 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04e30859-40e3-412a-889d-ec3eb8f7a1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Evaluation and Saving\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CNN Model Evaluation:\n",
      "Test Accuracy: 0.8764\n",
      "\n",
      "New CNN model saved successfully.\n",
      "Scaler and label encoder saved.\n",
      "\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Evaluation and Saving ---\n",
    "\n",
    "print(\"Step 3: Evaluation and Saving\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(\"\\nCNN Model Evaluation:\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the entire model\n",
    "model.save(\"genre_classifier_cnn_model.h5\")\n",
    "print(\"\\nNew CNN model saved successfully.\")\n",
    "\n",
    "# Also save the scaler and label encoder, as they are still needed for new predictions\n",
    "joblib.dump(scaler, \"scaler_cnn.pkl\")\n",
    "joblib.dump(label_encoder, \"label_encoder_cnn.pkl\")\n",
    "print(\"Scaler and label encoder saved.\")\n",
    "print(\"\\n\" + \"=\" * 40 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ea6d1c5-3604-4bb9-a9ee-f9281b92590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Prediction on a New Audio File\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Prediction on a New Audio File ---\n",
    "\n",
    "print(\"Step 4: Prediction on a New Audio File\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load the saved model and other objects\n",
    "model_loaded = tf.keras.models.load_model(\"genre_classifier_cnn_model.h5\")\n",
    "scaler_loaded = joblib.load(\"scaler_cnn.pkl\")\n",
    "label_encoder_loaded = joblib.load(\"label_encoder_cnn.pkl\")\n",
    "\n",
    "def extract_all_features(file_path):\n",
    "    \"\"\"\n",
    "    Extracts all the necessary features from an audio file to match the training data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, duration=30)\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        for i in range(mfccs.shape[0]):\n",
    "            features[f'mfcc{i+1}_mean'] = np.mean(mfccs[i])\n",
    "            features[f'mfcc{i+1}_var'] = np.var(mfccs[i])\n",
    "            \n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        features['chroma_stft_mean'] = np.mean(chroma)\n",
    "        features['chroma_stft_var'] = np.var(chroma)\n",
    "        \n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        features['rms_mean'] = np.mean(rms)\n",
    "        features['rms_var'] = np.var(rms)\n",
    "        \n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        features['spectral_centroid_mean'] = np.mean(spec_cent)\n",
    "        features['spectral_centroid_var'] = np.var(spec_cent)\n",
    "        \n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        features['spectral_bandwidth_mean'] = np.mean(spec_bw)\n",
    "        features['spectral_bandwidth_var'] = np.var(spec_bw)\n",
    "\n",
    "\n",
    "\n",
    "        spec_roll = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        features['spectral_rolloff_mean'] = np.mean(spec_roll)\n",
    "        features['spectral_rolloff_var'] = np.var(spec_roll)\n",
    "        \n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        features['zero_crossing_rate_mean'] = np.mean(zcr)\n",
    "        features['zero_crossing_rate_var'] = np.var(zcr)\n",
    "        \n",
    "        y_harm, y_perc = librosa.effects.hpss(y)\n",
    "        features['harmony_mean'] = np.mean(y_harm)\n",
    "        features['harmony_var'] = np.var(y_harm)\n",
    "        features['perceptr_mean'] = np.mean(y_perc)\n",
    "        features['perceptr_var'] = np.var(y_perc)\n",
    "        \n",
    "        tempo = librosa.beat.tempo(y=y, sr=sr)[0]\n",
    "        features['tempo'] = tempo\n",
    "\n",
    "        feature_names = pd.read_csv(\"features_3_sec.csv\").drop(columns=['filename', 'label']).columns.tolist()\n",
    "        \n",
    "        new_data_dict = {name: [features.get(name, 0)] for name in feature_names}\n",
    "        \n",
    "        new_data_df = pd.DataFrame(new_data_dict)\n",
    "\n",
    "        return new_data_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_path}. Error: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92912b39-7c6a-4dc4-86e7-a8018ed21c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from: C:\\Users\\HP\\Downloads\\archive (10)\\Data\\genres_original\\blues\\blues.00001.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_5272\\253611415.py:57: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  tempo = librosa.beat.tempo(y=y, sr=sr)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "\n",
      "The predicted genre for the audio file is: jazz\n"
     ]
    }
   ],
   "source": [
    "# Make sure to provide a valid path to an audio file from your dataset.\n",
    "new_audio_file = r\"C:\\Users\\HP\\Downloads\\archive (10)\\Data\\genres_original\\blues\\blues.00001.wav\"\n",
    "\n",
    "print(f\"Extracting features from: {new_audio_file}\")\n",
    "new_features_df = extract_all_features(new_audio_file)\n",
    "\n",
    "\n",
    "if new_features_df is not None:\n",
    "    features_scaled = scaler_loaded.transform(new_features_df)\n",
    "    features_reshaped = np.expand_dims(features_scaled, axis=2)\n",
    "    \n",
    "    predictions = model_loaded.predict(features_reshaped)\n",
    "    pred_label_num = np.argmax(predictions)\n",
    "    pred_label = label_encoder_loaded.inverse_transform([pred_label_num])[0]\n",
    "    \n",
    "    print(f\"\\nThe predicted genre for the audio file is: {pred_label}\")\n",
    "else:\n",
    "    print(\"Could not make a prediction due to a feature extraction error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21be16-00ce-4394-bf8c-66465686e7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
